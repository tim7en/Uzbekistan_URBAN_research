"""
Diagnostic script to analyze hazard score and vulnerability calculations
Helps identify potential issues with climate hazard and vulnerability scoring
"""

import pandas as pd
import numpy as np
from pathlib import Path
from services.climate_data_loader import ClimateDataLoader
from services.climate_risk_assessment import IPCCRiskAssessmentService


def analyze_hazard_vulnerability():
    """Analyze hazard and vulnerability calculation components for all cities"""
    
    # Load data using same path detection
    repo_root = Path(__file__).resolve().parent
    candidates = [
        repo_root / "suhi_analysis_output",
        repo_root / "reports",
        Path.cwd() / "suhi_analysis_output", 
        Path.cwd() / "reports",
        repo_root
    ]
    
    base_path = None
    for c in candidates:
        if c.exists():
            base_path = c
            break
    
    if base_path is None:
        base_path = repo_root
        print(f"Warning: no suhi_analysis_output/reports folder found; using repo root: {base_path}")
    else:
        print(f"Using data base_path: {base_path}")
    
    # Initialize services
    data_loader = ClimateDataLoader(str(base_path))
    risk_assessor = IPCCRiskAssessmentService(data_loader)
    
    # Load data
    data = data_loader.load_all_data()
    
    print("\n" + "="*100)
    print("HAZARD SCORE & VULNERABILITY ANALYSIS")
    print("="*100)
    
    # Analyze each city's hazard components
    cities = list(data['population_data'].keys())
    analysis_data = []
    
    for city in cities:
        print(f"\nAnalyzing {city}...")
        
        # Get temperature data for hazard analysis
        temp_data = data['temperature_data'].get(city, {})
        suhi_data = data['suhi_data'].get(city, {})
        
        hazard_components = {
            'city': city,
            'has_temp_data': bool(temp_data),
            'has_suhi_data': bool(suhi_data),
            'temp_years': len(temp_data.keys()) if temp_data else 0,
            'suhi_years': len(suhi_data.keys()) if suhi_data else 0
        }
        
        # Analyze temperature-based hazard if available
        if temp_data:
            years = sorted(temp_data.keys())
            if years:
                latest_year = years[-1]
                latest_stats = temp_data[latest_year]
                summer_stats = latest_stats.get('summer_season_summary', {})
                
                # Extract from actual data structure
                urban_day = summer_stats.get('urban', {}).get('day', {})
                urban_night = summer_stats.get('urban', {}).get('night', {})
                
                hazard_components.update({
                    'latest_year': latest_year,
                    'mean_summer_temp': urban_day.get('mean', 0),
                    'max_summer_temp': urban_day.get('max', 0),
                    'p90_summer_temp': urban_day.get('p90', 0),
                    'night_mean_temp': urban_night.get('mean', 0)
                })
                
                # Calculate temperature trend using corrected field names
                temp_trends = []
                for temp_type in ['day', 'night']:
                    values = []
                    years_list = []
                    for year in years:
                        year_data = temp_data[year]
                        summer_data = year_data.get('summer_season_summary', {})
                        urban_data = summer_data.get('urban', {})
                        temp_data_type = urban_data.get(temp_type, {})
                        temp_val = temp_data_type.get('mean')
                        
                        if temp_val is not None:
                            values.append(temp_val)
                            years_list.append(int(year))
                    
                    if len(values) >= 3:
                        try:
                            trend = np.polyfit(years_list, values, 1)[0]
                            temp_trends.append(trend)
                        except:
                            temp_trends.append(0.0)
                
                hazard_components['temp_trend'] = np.mean(temp_trends) if temp_trends else 0.0
        
        # Analyze SUHI-based hazard if temperature data unavailable
        elif suhi_data:
            years = sorted([int(y) for y in suhi_data.keys()])
            if years:
                latest_year = str(years[-1])
                stats = suhi_data[latest_year].get('stats', {})
                hazard_components.update({
                    'current_suhi': stats.get('suhi_night', 0),
                    'night_urban_temp': stats.get('night_urban_mean', 0)
                })
        
        # Calculate actual hazard score
        hazard_score = risk_assessor.calculate_hazard_score(city)
        hazard_components['hazard_score'] = hazard_score
        
        # Analyze vulnerability components
        population_data = data['population_data'].get(city)
        if population_data:
            # Economic vulnerability
            gdp_vulnerability = data_loader.pct_norm(
                data['cache']['gdp'], 
                population_data.gdp_per_capita_usd, 
                invert=True
            )
            
            # Built area vulnerability
            built_vulnerability = 0.0
            for lulc_city in data['lulc_data']:
                if lulc_city.get('city') == city:
                    areas = lulc_city.get('areas_m2', {})
                    if areas:
                        years = sorted([int(y) for y in areas.keys()])
                        if years:
                            latest_year = str(years[-1])
                            built_pct = areas[latest_year].get('Built_Area', {}).get('percentage', 0)
                            built_vulnerability = data_loader.pct_norm(
                                data['cache']['built_pct'], built_pct
                            )
                    break
            
            # Green space vulnerability
            green_vulnerability = 0.0
            spatial_city_data = data['spatial_data'].get('per_year', {}).get(city, {})
            if spatial_city_data:
                years = sorted([int(y) for y in spatial_city_data.keys()])
                if years:
                    latest_year = str(years[-1])
                    veg_access = spatial_city_data[latest_year].get('vegetation_accessibility', {}).get('city', {}).get('mean', 0)
                    veg_access_capped = min(100.0, max(0.0, veg_access))
                    green_vulnerability = max(0.0, 1.0 - (veg_access_capped / 100))
            
            hazard_components.update({
                'gdp_per_capita': population_data.gdp_per_capita_usd,
                'gdp_vulnerability': gdp_vulnerability,
                'built_vulnerability': built_vulnerability,
                'green_vulnerability': green_vulnerability,
                'total_vulnerability': risk_assessor.calculate_vulnerability_score(city)
            })
        
        analysis_data.append(hazard_components)
    
    df = pd.DataFrame(analysis_data)
    df = df.sort_values('hazard_score', ascending=False)
    
    print(f"\nHAZARD SCORE ANALYSIS:")
    print(f"  Hazard Score Range: {df['hazard_score'].min():.3f} - {df['hazard_score'].max():.3f}")
    print(f"  Cities with hazard_score = 0: {len(df[df['hazard_score'] == 0])}")
    print(f"  Cities with temperature data: {df['has_temp_data'].sum()}")
    print(f"  Cities with SUHI data: {df['has_suhi_data'].sum()}")
    
    # Show top hazard cities
    print(f"\nTOP 5 CITIES BY HAZARD SCORE:")
    hazard_cols = ['city', 'hazard_score', 'has_temp_data', 'mean_summer_temp', 'max_summer_temp', 
                   'p90_summer_temp', 'temp_trend']
    available_cols = [col for col in hazard_cols if col in df.columns]
    print(df[available_cols].head().to_string(index=False, float_format='%.3f'))
    
    # Check for data issues
    print(f"\nPOTENTIAL HAZARD ISSUES:")
    
    # Cities with zero hazard scores
    zero_hazard = df[df['hazard_score'] == 0.0]
    if not zero_hazard.empty:
        print(f"  Cities with ZERO hazard score:")
        for _, row in zero_hazard.iterrows():
            print(f"    {row['city']}: temp_data={row['has_temp_data']}, suhi_data={row['has_suhi_data']}")
    
    # Cities with unrealistic temperature values
    if 'mean_summer_temp' in df.columns:
        extreme_temps = df[(df['mean_summer_temp'] > 45) | (df['mean_summer_temp'] < 15)]
        if not extreme_temps.empty:
            print(f"  Cities with extreme summer temperatures:")
            for _, row in extreme_temps.iterrows():
                print(f"    {row['city']}: {row['mean_summer_temp']:.1f}°C mean summer temp")
    
    print(f"\nVULNERABILITY ANALYSIS:")
    vuln_cols = ['city', 'total_vulnerability', 'gdp_vulnerability', 'built_vulnerability', 'green_vulnerability']
    available_vuln_cols = [col for col in vuln_cols if col in df.columns]
    print(f"TOP 5 MOST VULNERABLE CITIES:")
    print(df.sort_values('total_vulnerability', ascending=False)[available_vuln_cols].head().to_string(index=False, float_format='%.3f'))
    
    # Check cache ranges
    print(f"\nDATA CACHE RANGES:")
    for cache_key in ['gdp', 'built_pct', 'population', 'density']:
        if cache_key in data['cache']:
            cache_values = data['cache'][cache_key]
            print(f"  {cache_key}: {min(cache_values):.2f} - {max(cache_values):.2f}")
    
    # Save detailed breakdown
    output_file = base_path / "climate_assessment" / "hazard_vulnerability_breakdown.csv"
    df.to_csv(output_file, index=False)
    print(f"\n✓ Saved detailed breakdown to: {output_file}")
    
    print("="*100)
    
    return df


if __name__ == "__main__":
    analyze_hazard_vulnerability()
