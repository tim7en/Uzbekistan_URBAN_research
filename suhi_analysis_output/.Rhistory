summer_lst_mean  = safe_num(s$summer_lst_mean),
winter_lst_mean  = safe_num(s$winter_lst_mean),
lst_change_mean  = safe_num(s$lst_change_mean),
summer_biomass_t_per_ha = safe_num(s$summer_biomass_t_per_ha),
winter_biomass_t_per_ha = safe_num(s$winter_biomass_t_per_ha),
biomass_change_t_per_ha = safe_num(s$biomass_change_t_per_ha)
)
}
# ---- Flatten ALL cities/years (preallocated, year-filtered) ----
city_names <- names(raw)
# keep only numeric year keys (e.g., "2016" .. "2024")
get_year_keys <- function(city_obj) {
y <- names(city_obj)
y[grepl("^[0-9]{4}$", y)]
}
total_rows <- sum(vapply(city_names, function(c) length(get_year_keys(raw[[c]])), integer(1)))
stats_list <- vector("list", total_rows)
k <- 0L
for (city in city_names) {
years <- get_year_keys(raw[[city]])
for (year in years) {
k <- k + 1L
stats_list[[k]] <- flatten_city_year(city, year, raw[[city]][[year]])
}
}
# Trim in case of any skipped years
if (k < total_rows) stats_list <- stats_list[seq_len(k)]
stats_wide <- dplyr::bind_rows(stats_list) |>
dplyr::arrange(city, year)
# ---- Result ----
print(utils::head(stats_wide, 20))
cat("\nRows:", nrow(stats_wide),
"  Columns:", ncol(stats_wide),
"  Cities:", length(unique(stats_wide$city)),
"  Years:", paste(range(stats_wide$year, na.rm = TRUE), collapse = "–"), "\n")
# ---- Optional: save to CSV ----
dir.create("out", showWarnings = FALSE)
utils::write.csv(stats_wide, "out/stats_wide.csv", row.names = FALSE)
message("Saved: out/stats_wide.csv")
# 1) Make sure every city has rows for all years (2016–2024)
years_full <- 2016:2024
stats_complete <- stats_wide %>%
group_by(city) %>%
complete(year = years_full) %>%
arrange(city, year, .by_group = TRUE) %>%
ungroup()
# 2) Filler: linear interpolation for interior gaps; OLS for edges
fill_city_series <- function(df_city) {
num_cols <- setdiff(names(df_city), c("city", "year"))
for (col in num_cols) {
y <- df_city[[col]]
# (a) Linear interpolation for interior points
if (sum(!is.na(y)) >= 2) {
interp <- approx(
x = df_city$year[!is.na(y)],
y = y[!is.na(y)],
xout = df_city$year,
method = "linear",
rule = 1,         # NA outside the observed range (we'll handle next)
ties = "ordered"
)$y
} else {
interp <- y
}
# (b) Edge years still NA → predict from per-city linear model
still_na <- is.na(interp)
if (any(still_na)) {
if (sum(!is.na(y)) >= 2) {
fit <- lm(y ~ year, data = df_city)
interp[still_na] <- predict(fit, newdata = df_city[still_na, , drop = FALSE])
} else if (sum(!is.na(y)) == 1) {
# If only one observed point exists, use that value for all missing
interp[still_na] <- y[which(!is.na(y))[1]]
}
# If no observations exist at all for this column/city, leave as NA
}
df_city[[col]] <- interp
}
df_city
}
stats_filled <- stats_complete %>%
group_by(city) %>%
group_modify(~ fill_city_series(.x)) %>%
ungroup()
# Now use `stats_filled` instead of `stats_wide`
print(head(stats_filled, 20))
View(stats_filled)
# ==============================
# Convert auxiliary_batch_results.json → dataframe (fixed & robust)
# ==============================
# ---- Packages ----
pkgs <- c("jsonlite","dplyr","tibble")
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
# ---- Safety / options ----
try(setTimeLimit(cpu = Inf, elapsed = Inf, transient = TRUE), silent = TRUE)
options(stringsAsFactors = FALSE)
# ---- Helpers ----
`%||%` <- function(x, y) if (is.null(x)) y else x
safe_num <- function(x) if (is.null(x)) NA_real_ else suppressWarnings(as.numeric(x))
# ---- Locate & load JSON ----
# Set path_to_json manually if you prefer; otherwise we auto-detect:
if (!exists("path_to_json")) {
candidates <- c(
"reports/auxiliary_batch_results.json",
"auxiliary_batch_results.json",
"D:/Dev/Uzbekistan_URBAN_research/suhi_analysis_output/reports/auxiliary_batch_results.json"
)
path_to_json <- candidates[file.exists(candidates)][1]
}
if (is.na(path_to_json)) stop("JSON not found. Set `path_to_json` to your file path.")
message("Using file: ", path_to_json)
raw <- jsonlite::fromJSON(path_to_json, simplifyVector = FALSE)
# ---- Flatten one city-year ----
flatten_city_year <- function(city_name, year_key, x) {
s <- x$stats %||% list()
tibble::tibble(
city  = city_name,
year  = as.integer(year_key),
summer_ndvi_mean = safe_num(s$summer_ndvi_mean),
winter_ndvi_mean = safe_num(s$winter_ndvi_mean),
ndvi_change_mean = safe_num(s$ndvi_change_mean),
summer_evi_mean  = safe_num(s$summer_evi_mean),
winter_evi_mean  = safe_num(s$winter_evi_mean),
evi_change_mean  = safe_num(s$evi_change_mean),
summer_lst_mean  = safe_num(s$summer_lst_mean),
winter_lst_mean  = safe_num(s$winter_lst_mean),
lst_change_mean  = safe_num(s$lst_change_mean),
summer_biomass_t_per_ha = safe_num(s$summer_biomass_t_per_ha),
winter_biomass_t_per_ha = safe_num(s$winter_biomass_t_per_ha),
biomass_change_t_per_ha = safe_num(s$biomass_change_t_per_ha)
)
}
# ---- Flatten ALL cities/years (preallocated, year-filtered) ----
city_names <- names(raw)
# keep only numeric year keys (e.g., "2016" .. "2024")
get_year_keys <- function(city_obj) {
y <- names(city_obj)
y[grepl("^[0-9]{4}$", y)]
}
total_rows <- sum(vapply(city_names, function(c) length(get_year_keys(raw[[c]])), integer(1)))
stats_list <- vector("list", total_rows)
k <- 0L
for (city in city_names) {
years <- get_year_keys(raw[[city]])
for (year in years) {
k <- k + 1L
stats_list[[k]] <- flatten_city_year(city, year, raw[[city]][[year]])
}
}
# Trim in case of any skipped years
if (k < total_rows) stats_list <- stats_list[seq_len(k)]
stats_wide <- dplyr::bind_rows(stats_list) |>
dplyr::arrange(city, year)
# ---- Result ----
print(utils::head(stats_wide, 20))
cat("\nRows:", nrow(stats_wide),
"  Columns:", ncol(stats_wide),
"  Cities:", length(unique(stats_wide$city)),
"  Years:", paste(range(stats_wide$year, na.rm = TRUE), collapse = "–"), "\n")
# 1) Make sure every city has rows for all years (2016–2024)
years_full <- 2016:2024
stats_complete <- stats_wide %>%
group_by(city) %>%
complete(year = years_full) %>%
arrange(city, year, .by_group = TRUE) %>%
ungroup()
# 2) Filler: linear interpolation for interior gaps; OLS for edges
fill_city_series <- function(df_city) {
num_cols <- setdiff(names(df_city), c("city", "year"))
for (col in num_cols) {
y <- df_city[[col]]
# (a) Linear interpolation for interior points
if (sum(!is.na(y)) >= 2) {
interp <- approx(
x = df_city$year[!is.na(y)],
y = y[!is.na(y)],
xout = df_city$year,
method = "linear",
rule = 1,         # NA outside the observed range (we'll handle next)
ties = "ordered"
)$y
} else {
interp <- y
}
# (b) Edge years still NA → predict from per-city linear model
still_na <- is.na(interp)
if (any(still_na)) {
if (sum(!is.na(y)) >= 2) {
fit <- lm(y ~ year, data = df_city)
interp[still_na] <- predict(fit, newdata = df_city[still_na, , drop = FALSE])
} else if (sum(!is.na(y)) == 1) {
# If only one observed point exists, use that value for all missing
interp[still_na] <- y[which(!is.na(y))[1]]
}
# If no observations exist at all for this column/city, leave as NA
}
df_city[[col]] <- interp
}
df_city
}
stats_filled <- stats_complete %>%
group_by(city) %>%
group_modify(~ fill_city_series(.x)) %>%
ungroup()
# Now use `stats_filled` instead of `stats_wide`
print(head(stats_filled, 20))
# ---- Optional: save to CSV ----
dir.create("out", showWarnings = FALSE)
utils::write.csv(stats_filled, "out/stats_filled.csv", row.names = FALSE)
message("Saved: out/stats_filled.csv")
# ==========================================================
# Comprehensive Urban RS Analysis from stats_filled.csv
# Exports: ./out/*.csv + ./out/*.png
# ==========================================================
# -------- Packages --------
pkgs <- c(
"readr","dplyr","tidyr","stringr","purrr",
"ggplot2","imputeTS","broom","scales","forcats","lubridate","ggcorrplot"
)
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
dir.create("out", showWarnings = FALSE)
# -------- Config --------
path <- "out/stats_filled.csv"
numeric_cols <- c(
"year",
"summer_ndvi_mean","winter_ndvi_mean","ndvi_change_mean",
"summer_evi_mean","winter_evi_mean","evi_change_mean",
"summer_lst_mean","winter_lst_mean","lst_change_mean",
"summer_biomass_t_per_ha","winter_biomass_t_per_ha","biomass_change_t_per_ha"
)
# -------- 1) Load --------
raw <- readr::read_csv(path, show_col_types = FALSE, guess_max = 10000) |>
janitor::clean_names()
# Ensure required columns exist
missing_cols <- setdiff(c("city", numeric_cols), names(raw))
if (length(missing_cols)) stop("Missing columns: ", paste(missing_cols, collapse=", "))
# Coerce numerics safely
dat <- raw |>
mutate(across(all_of(numeric_cols), ~ suppressWarnings(as.numeric(.x))))
# -------- 2) Validate ranges & flag outliers (set to NA) --------
# Heuristic physical ranges (tune if needed)
rng <- list(
ndvi = c(-0.2, 0.9),
evi  = c(-0.2, 1.2),
lst  = c(-50, 70),          # °C or K-273-ish; adjust if needed
bio  = c(0, 50)             # t/ha (order-of-magnitude guardrail)
)
is_out <- function(x, lo, hi) ifelse(is.na(x), NA, x < lo | x > hi)
dat_clean <- dat |>
mutate(
ndvi_s_out_s  = is_out(summer_ndvi_mean, rng$ndvi[1], rng$ndvi[2]),
ndvi_s_out_w  = is_out(winter_ndvi_mean, rng$ndvi[1], rng$ndvi[2]),
evi_out_s     = is_out(summer_evi_mean,  rng$evi[1],  rng$evi[2]),
evi_out_w     = is_out(winter_evi_mean,  rng$evi[1],  rng$evi[2]),
lst_out_s     = is_out(summer_lst_mean,  rng$lst[1],  rng$lst[2]),
lst_out_w     = is_out(winter_lst_mean,  rng$lst[1],  rng$lst[2]),
bio_out_s     = is_out(summer_biomass_t_per_ha, rng$bio[1], rng$bio[2]),
bio_out_w     = is_out(winter_biomass_t_per_ha, rng$bio[1], rng$bio[2])
) |>
mutate(
across(
c(summer_ndvi_mean,winter_ndvi_mean,summer_evi_mean,winter_evi_mean,
summer_lst_mean,winter_lst_mean,summer_biomass_t_per_ha,winter_biomass_t_per_ha),
\(x, nm = cur_column()){
lohi <- dplyr::case_when(
stringr::str_detect(nm,"ndvi") ~ list(rng$ndvi),
stringr::str_detect(nm,"evi")  ~ list(rng$evi),
stringr::str_detect(nm,"lst")  ~ list(rng$lst),
stringr::str_detect(nm,"biomass") ~ list(rng$bio),
TRUE ~ list(c(-Inf, Inf))
)[[1]]
replace(x, x < lohi[1] | x > lohi[2], NA_real_)
}
)
) |>
# Recompute change columns after NAing extremes (summer - winter if present)
mutate(
ndvi_change_mean    = summer_ndvi_mean    - winter_ndvi_mean,
evi_change_mean     = summer_evi_mean     - winter_evi_mean,
lst_change_mean     = summer_lst_mean     - winter_lst_mean,
biomass_change_t_per_ha = summer_biomass_t_per_ha - winter_biomass_t_per_ha
)
readr::write_csv(dat_clean, "out/01_clean_deglitched.csv")
# -------- 3) Impute NAs per city & variable --------
# Strategy: linear interpolation over year → Kalman (ARIMA) fallback
vars_to_impute <- c(
"summer_ndvi_mean","winter_ndvi_mean","ndvi_change_mean",
"summer_evi_mean","winter_evi_mean","evi_change_mean",
"summer_lst_mean","winter_lst_mean","lst_change_mean",
"summer_biomass_t_per_ha","winter_biomass_t_per_ha","biomass_change_t_per_ha"
)
impute_series <- function(y) {
z <- y
if (all(is.na(z))) return(z)
z <- imputeTS::na_interpolation(z, option = "linear")
if (any(is.na(z))) z <- imputeTS::na_kalman(z, model = "auto.arima")
z
}
dat_imp <- dat_clean |>
arrange(city, year) |>
group_by(city) |>
mutate(across(all_of(vars_to_impute), impute_series)) |>
ungroup()
# ==========================================================
# Comprehensive Analysis of Vegetation, Heat, and Biomass
# Dataset: stats_filled.csv (already filled)
# ==========================================================
# -------- Packages --------
pkgs <- c(
"readr","dplyr","tidyr","ggplot2",
"broom","scales","forcats","ggcorrplot"
)
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
# -------- Load data --------
df <- read_csv("stats_filled.csv")
# ==========================================================
# Comprehensive Analysis of Vegetation, Heat, and Biomass
# Dataset: stats_filled.csv (already filled)
# ==========================================================
# -------- Packages --------
pkgs <- c(
"readr","dplyr","tidyr","ggplot2",
"broom","scales","forcats","ggcorrplot"
)
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
# -------- Load data --------
df <- read_csv("out/stats_filled.csv")
# Check structure
glimpse(df)
# -------- Summary stats --------
summary_stats <- df %>%
group_by(city) %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))
write_csv(summary_stats, "out/summary_stats.csv")
# -------- Trend analysis (linear regression by city) --------
trend_models <- df %>%
group_by(city) %>%
do({
mod <- lm(summer_ndvi_mean ~ year, data = .)
tidy(mod)
})
write_csv(trend_models, "out/ndvi_trends.csv")
# -------- Correlation analysis --------
cor_vars <- c("summer_ndvi_mean","winter_ndvi_mean","ndvi_change_mean",
"summer_evi_mean","winter_evi_mean","evi_change_mean",
"summer_lst_mean","winter_lst_mean","lst_change_mean",
"summer_biomass_t_per_ha","winter_biomass_t_per_ha","biomass_change_t_per_ha")
cor_matrix <- cor(df[cor_vars], use = "pairwise.complete.obs")
png("out/correlation_heatmap.png", width = 900, height = 700)
ggcorrplot(cor_matrix, hc.order = TRUE, type = "lower",
lab = TRUE, lab_size = 3, colors = c("blue","white","red")) +
ggtitle("Correlation Heatmap: Vegetation, LST, Biomass")
dev.off()
# -------- Time series plots by city --------
plot_vars <- c("summer_ndvi_mean","summer_evi_mean","summer_lst_mean","summer_biomass_t_per_ha")
for (v in plot_vars) {
p <- ggplot(df, aes(x = year, y = .data[[v]], color = city, group = city)) +
geom_line(size = 1) + geom_point() +
theme_minimal() +
labs(title = paste("Trend of", v, "by City"),
y = v, x = "Year")
ggsave(filename = paste0("out/", v, "_trend.png"), plot = p, width = 10, height = 6)
}
# -------- Change distributions --------
change_vars <- c("ndvi_change_mean","evi_change_mean","lst_change_mean","biomass_change_t_per_ha")
for (v in change_vars) {
p <- ggplot(df, aes(x = city, y = .data[[v]], fill = city)) +
geom_boxplot() +
theme_minimal() +
coord_flip() +
labs(title = paste("Distribution of", v, "across Cities"),
y = v, x = "City")
ggsave(filename = paste0("out/", v, "_distribution.png"), plot = p, width = 9, height = 6)
}
message("Analysis complete. Outputs saved in 'out/' folder.")
# ==========================================================
# Comprehensive Analysis of Vegetation, Heat, and Biomass
# Dataset: stats_filled.csv (already filled)
# ==========================================================
# -------- Packages --------
pkgs <- c(
"readr","dplyr","tidyr","ggplot2",
"broom","scales","forcats","ggcorrplot"
)
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
# -------- Load data --------
df <- read_csv("out/stats_filled.csv")
# Check structure
glimpse(df)
clean_outliers <- function(x) {
# remove impossible values (< -1 or > 1 are invalid for vegetation indices)
x[x < -1 | x > 1] <- NA
return(x)
}
df <- df %>%
mutate(
summer_evi_mean  = clean_outliers(summer_evi_mean),
winter_evi_mean  = clean_outliers(winter_evi_mean),
evi_change_mean  = clean_outliers(evi_change_mean)
)
# Optionally: drop rows with NA after cleaning
# df <- df %>% drop_na(summer_evi_mean, winter_evi_mean, evi_change_mean)
# Or: interpolate missing values (since your file is already "filled")
df <- df %>%
group_by(city) %>%
arrange(year, .by_group = TRUE) %>%
mutate(
summer_evi_mean = zoo::na.approx(summer_evi_mean, na.rm = FALSE),
winter_evi_mean = zoo::na.approx(winter_evi_mean, na.rm = FALSE),
evi_change_mean = zoo::na.approx(evi_change_mean, na.rm = FALSE)
) %>%
ungroup()
# -------- Summary stats --------
summary_stats <- df %>%
group_by(city) %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))
write_csv(summary_stats, "out/summary_stats.csv")
# ==========================================================
# Comprehensive Analysis of Vegetation, Heat, and Biomass
# Dataset: stats_filled.csv (already filled)
# ==========================================================
# -------- Packages --------
pkgs <- c(
"readr","dplyr","tidyr","ggplot2",
"broom","scales","forcats","ggcorrplot"
)
to_install <- setdiff(pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dep = TRUE)
invisible(lapply(pkgs, library, character.only = TRUE))
# -------- Load data --------
df <- read_csv("out/stats_filled.csv")
# Check structure
glimpse(df)
clean_outliers <- function(x) {
# remove impossible values (< -1 or > 1 are invalid for vegetation indices)
x[x < -1 | x > 1] <- NA
return(x)
}
df <- df %>%
mutate(
summer_evi_mean  = clean_outliers(summer_evi_mean),
winter_evi_mean  = clean_outliers(winter_evi_mean),
evi_change_mean  = clean_outliers(evi_change_mean)
)
# Optionally: drop rows with NA after cleaning
# df <- df %>% drop_na(summer_evi_mean, winter_evi_mean, evi_change_mean)
# Or: interpolate missing values (since your file is already "filled")
df <- df %>%
group_by(city) %>%
arrange(year, .by_group = TRUE) %>%
mutate(
summer_evi_mean = zoo::na.approx(summer_evi_mean, na.rm = FALSE),
winter_evi_mean = zoo::na.approx(winter_evi_mean, na.rm = FALSE),
evi_change_mean = zoo::na.approx(evi_change_mean, na.rm = FALSE)
) %>%
ungroup()
# -------- Summary stats --------
summary_stats <- df %>%
group_by(city) %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))
write_csv(summary_stats, "out/summary_stats.csv")
# -------- Trend analysis (linear regression by city) --------
trend_models <- df %>%
group_by(city) %>%
do({
mod <- lm(summer_ndvi_mean ~ year, data = .)
tidy(mod)
})
write_csv(trend_models, "out/ndvi_trends.csv")
# -------- Correlation analysis --------
cor_vars <- c("summer_ndvi_mean","winter_ndvi_mean","ndvi_change_mean",
"summer_evi_mean","winter_evi_mean","evi_change_mean",
"summer_lst_mean","winter_lst_mean","lst_change_mean",
"summer_biomass_t_per_ha","winter_biomass_t_per_ha","biomass_change_t_per_ha")
cor_matrix <- cor(df[cor_vars], use = "pairwise.complete.obs")
png("out/correlation_heatmap.png", width = 900, height = 700)
ggcorrplot(cor_matrix, hc.order = TRUE, type = "lower",
lab = TRUE, lab_size = 3, colors = c("blue","white","red")) +
ggtitle("Correlation Heatmap: Vegetation, LST, Biomass")
dev.off()
# -------- Time series plots by city --------
plot_vars <- c("summer_ndvi_mean","summer_evi_mean","summer_lst_mean","summer_biomass_t_per_ha")
for (v in plot_vars) {
p <- ggplot(df, aes(x = year, y = .data[[v]], color = city, group = city)) +
geom_line(size = 1) + geom_point() +
theme_minimal() +
labs(title = paste("Trend of", v, "by City"),
y = v, x = "Year")
ggsave(filename = paste0("out/", v, "_trend.png"), plot = p, width = 10, height = 6)
}
# -------- Change distributions --------
change_vars <- c("ndvi_change_mean","evi_change_mean","lst_change_mean","biomass_change_t_per_ha")
for (v in change_vars) {
p <- ggplot(df, aes(x = city, y = .data[[v]], fill = city)) +
geom_boxplot() +
theme_minimal() +
coord_flip() +
labs(title = paste("Distribution of", v, "across Cities"),
y = v, x = "City")
ggsave(filename = paste0("out/", v, "_distribution.png"), plot = p, width = 9, height = 6)
}
message("Analysis complete. Outputs saved in 'out/' folder.")
